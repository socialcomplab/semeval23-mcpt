{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import gc\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import trange\n",
    "from typing import Iterator, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, Sampler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mcpt.contrastlearning import DataManager, TrainerA, WeightedCosineSimilarityLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "#data_path = '../input/semeval/data'\n",
    "DEV = True\n",
    "#model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "#model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "#model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "model_sampler = 'random'\n",
    "N_EPOCHS = 1000\n",
    "N_FINETUNE_EPOCHS = 50\n",
    "N_EPOCHS_BEFORE_FINETUNE = 50\n",
    "N_POST_FINETUNE_EPOCHS = 50\n",
    "MODEL_BATCH_SIZE = 26\n",
    "HEAD_BATCH_SIZE = 200\n",
    "MIN_SAMPLES_FROM_CLASS = 2\n",
    "HEAD_LR = 1e-3\n",
    "HEAD_GAMMA = .99\n",
    "MODEL_LR = 2e-5\n",
    "BETA = 0.01\n",
    "MODEL_GAMMA = .98\n",
    "VALIDATE_EVERY = -1\n",
    "CHECKPOINT_EVERY = 10\n",
    "EARLIEST_CHECKPOINT = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "all_langs = ['en', 'ge', 'fr', 'it', 'ru', 'po']\n",
    "datamanager = DataManager(\n",
    "    tokenizer=tokenizer,\n",
    "    data_dir=data_path,\n",
    "    use_dev=DEV,\n",
    "    languages_for_head_eval=[],\n",
    "    languages_for_head_train=all_langs,\n",
    "    languages_for_contrastive=all_langs,\n",
    ")\n",
    "N_CLASSES = datamanager.num_classes\n",
    "metrics = list()\n",
    "reference_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = model.embeddings.word_embeddings.embedding_dim\n",
    "head = nn.Sequential(\n",
    "    nn.Linear(EMBEDDING_DIM, 256),\n",
    "    nn.Dropout(),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.Dropout(),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, N_CLASSES),\n",
    "    nn.Dropout(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_contrastive = datamanager.get_contrastive_dataset()\n",
    "dataset_head_train = datamanager.get_head_train_dataset()\n",
    "dataset_head_eval = datamanager.get_head_eval_dataset()\n",
    "trainer = TrainerA(\n",
    "    model=model,\n",
    "    head=head,\n",
    "    device=device,\n",
    "    head_loss=nn.BCEWithLogitsLoss(),\n",
    "    model_loss=WeightedCosineSimilarityLoss(N_CLASSES),\n",
    "    model_dataset=dataset_contrastive,\n",
    "    head_dataset=dataset_head_train,\n",
    "    eval_dataset=dataset_head_eval,        \n",
    "    n_classes=N_CLASSES,\n",
    "    model_loader_type=model_sampler,\n",
    "    train_head_batch_size=HEAD_BATCH_SIZE,\n",
    "    train_model_batch_size=MODEL_BATCH_SIZE,\n",
    "    head_lr=HEAD_LR,\n",
    "    model_lr=MODEL_LR,\n",
    "    head_gamma=HEAD_GAMMA,\n",
    "    model_gamma=MODEL_GAMMA,\n",
    "    beta=BETA,\n",
    "    min_samples_from_class=MIN_SAMPLES_FROM_CLASS,\n",
    "    validate_every_n_epochs=VALIDATE_EVERY,\n",
    "    checkpoint_every_n_epochs=CHECKPOINT_EVERY,\n",
    "    earliest_checkpoint=EARLIEST_CHECKPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reference_list.append(dataset_head_eval)\n",
    "trainer.train_head(N_EPOCHS_BEFORE_FINETUNE)\n",
    "trainer.train_joint(N_FINETUNE_EPOCHS)\n",
    "trainer.train_head(N_POST_FINETUNE_EPOCHS)\n",
    "#trainer.save_hparams('mpnet-multilang-body-hparams')\n",
    "#trainer.save_log_dict('mpnet-multilang-body-logdict')\n",
    "metrics.append(trainer.log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "langs = all_langs\n",
    "for lang in langs:\n",
    "    dataset_sanity = datamanager._get_single_named_dataset(lang, dev=True)\n",
    "    dataset_sanity = datamanager._preprocess_head_dataset(dataset_sanity)\n",
    "    embeddings = trainer.compute_embeddings(dataset_sanity)\n",
    "    predictions = trainer.predict(embeddings.tensors[0], 'cpu')\n",
    "    f1 = f1_score(dataset_sanity['labels'], predictions, average='micro')\n",
    "    print(lang, ': ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in all_langs:\n",
    "    print(f'Training {lang}')\n",
    "    datamanager = DataManager(\n",
    "        tokenizer=tokenizer,\n",
    "        data_dir=data_path,\n",
    "        use_dev=DEV,\n",
    "        languages_for_head_eval=[],\n",
    "        languages_for_head_train=[lang],\n",
    "        languages_for_contrastive=[lang],\n",
    "    )\n",
    "    dataset_contrastive = datamanager.get_contrastive_dataset()\n",
    "    dataset_head_train = datamanager.get_head_train_dataset()\n",
    "    dataset_head_eval = datamanager.get_head_eval_dataset()\n",
    "    trainer = TrainerA(\n",
    "        model=model,\n",
    "        head=head,\n",
    "        device=device,\n",
    "        head_loss=nn.BCEWithLogitsLoss(),\n",
    "        model_loss=WeightedCosineSimilarityLoss(N_CLASSES),\n",
    "        model_dataset=dataset_contrastive,\n",
    "        head_dataset=dataset_head_train,\n",
    "        eval_dataset=dataset_head_eval,        \n",
    "        n_classes=N_CLASSES,\n",
    "        model_loader_type=model_sampler,\n",
    "        train_head_batch_size=HEAD_BATCH_SIZE,\n",
    "        train_model_batch_size=MODEL_BATCH_SIZE,\n",
    "        head_lr=HEAD_LR,\n",
    "        model_lr=MODEL_LR,\n",
    "        head_gamma=HEAD_GAMMA,\n",
    "        model_gamma=MODEL_GAMMA,\n",
    "        beta=BETA,\n",
    "        min_samples_from_class=MIN_SAMPLES_FROM_CLASS,\n",
    "        validate_every_n_epochs=VALIDATE_EVERY,\n",
    "        checkpoint_every_n_epochs=1000,\n",
    "        earliest_checkpoint=1000,\n",
    "    )\n",
    "    trainer.load_from_checkpoint('joint_49')\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(EMBEDDING_DIM, 256),\n",
    "        nn.Dropout(),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.Dropout(),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, N_CLASSES),\n",
    "        nn.Dropout(),\n",
    "    )\n",
    "    head_optimizer = AdamW(head.parameters(), lr=HEAD_LR)\n",
    "    trainer.set_head(head, head_optimizer)\n",
    "\n",
    "    trainer.train_head(N_EPOCHS_BEFORE_FINETUNE)\n",
    "    trainer.train_joint(N_FINETUNE_EPOCHS)\n",
    "    trainer.train_head(N_POST_FINETUNE_EPOCHS)\n",
    "    \n",
    "    dataset_sanity = datamanager._get_single_named_dataset(lang, dev=True)\n",
    "    dataset_sanity = datamanager._preprocess_head_dataset(dataset_sanity)\n",
    "    embeddings = trainer.compute_embeddings(dataset_sanity)\n",
    "    predictions = trainer.predict(embeddings.tensors[0], 'cpu')\n",
    "    f1 = f1_score(dataset_sanity['labels'], predictions, average='micro')\n",
    "    print('  ', lang, ': ', f1)\n",
    "    \n",
    "    print('  Writing prediction file.')\n",
    "    datamanager.predict_and_write(\n",
    "        trainer,\n",
    "        articles_dir=f'../input/semeval/data/{lang}/test-articles-subtask-2',\n",
    "        output_file=f'predictions_{lang}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en', 'ge', 'fr', 'it', 'ru', 'po', 'ka', 'es', 'gr']\n",
    "for lang in langs:\n",
    "    datamanager.predict_and_write(\n",
    "        trainer,\n",
    "        articles_dir=f'../input/semeval/data/{lang}/test-articles-subtask-2',\n",
    "        output_file=f'predictions_{lang}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average Max MicroF1: {np.mean(np.array([np.max(m[\"microf1\"]) for m in metrics]))}')\n",
    "print(f'Average Max MacroF1: {np.mean(np.array([np.max(m[\"macrof1\"]) for m in metrics]))}')\n",
    "print(f'Average Max Train MicroF1: {np.mean(np.array([np.max(m[\"train_microf1\"]) for m in metrics]))}')\n",
    "print(f'Average Max Train MacroF1: {np.mean(np.array([np.max(m[\"train_macrof1\"]) for m in metrics]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_dict in metrics:\n",
    "    TrainerA.plot_metrics(log_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_scores = []\n",
    "ground = reference_list[1]['labels'].numpy()\n",
    "for KNN_preds in metrics[1]['KNNlogits']:\n",
    "    KNN_preds = torch.round(KNN_preds).numpy()\n",
    "    KNN_scores.append(f1_score(ground, KNN_preds, average='micro'))\n",
    "plt.plot(KNN_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer.log_dict['WCSL'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = INT2LABEL\n",
    "\n",
    "def per_label_f1(predictions, references):\n",
    "    f1 = f1_score(references, predictions, average=None)\n",
    "    print(\"f1:\", f1)\n",
    "    micro_f1 = f1_score(references, predictions, average=\"micro\")\n",
    "    print(\"micro-f1:\", micro_f1)\n",
    "    macro_f1 = f1_score(references, predictions, average=\"macro\")\n",
    "    print(\"macro-f1:\", macro_f1)\n",
    "\n",
    "    correct = []\n",
    "    label_names = []\n",
    "    for c in range(len(categories)):\n",
    "        correct.append(f1[c])\n",
    "        label_names.append(categories[c])\n",
    "    correct = np.array(correct)\n",
    "    label_names = np.array(label_names)\n",
    "    df_correct_pred = pd.DataFrame({\"f1_score\": correct, \"label_name\": label_names})\n",
    "\n",
    "    order = sorted(range(len(categories)), key=lambda i: f1[i])\n",
    "    return order, sns.barplot(x=\"f1_score\", y=\"label_name\", data=df_correct_pred, order=np.array(categories)[order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(predictions, references):\n",
    "    order, _ = per_label_f1(predictions, references)\n",
    "    plt.show()\n",
    "    \n",
    "    pred_heatmap = pd.DataFrame(predictions, columns=categories)\n",
    "    correct_predictions = references == predictions\n",
    "    false_predictions   = references != predictions\n",
    "    pred_heatmap[(correct_predictions & (predictions == 1))] = 3 # correct and one\n",
    "    pred_heatmap[(correct_predictions & (predictions == 0))] = 2 # correct and zero\n",
    "    pred_heatmap[(false_predictions & (predictions == 1))] = 1   # false and actually zero\n",
    "    pred_heatmap[(false_predictions & (predictions == 0))] = 0   # false and actually one\n",
    "    \n",
    "    pred_heatmap = pred_heatmap.iloc[:,order[::-1]]\n",
    "    pred_heatmap['false_predictions'] = false_predictions.sum(axis=1)\n",
    "    #pred_heatmap = pred_heatmap.sort_values(by='false_predictions', ascending=False)\n",
    "    pred_heatmap = pred_heatmap.sort_values(by=list(pred_heatmap.columns), ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=[14, 20])\n",
    "    cmap = sns.color_palette(\"coolwarm_r\", 4)\n",
    "    sns.heatmap(pred_heatmap.iloc[:,:-1], cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, m in zip(reference_list, metrics):\n",
    "    best_epoch = np.argmax(m['microf1'])\n",
    "    predictions = np.array([p.cpu().tolist() for p in m['predictions']][best_epoch])\n",
    "    references = r['labels'].numpy()\n",
    "    plot_all(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
